{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d04c6ea-0759-43ff-885a-64950a328efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ee237c-28d1-4ea6-810d-1215df3d6b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import os\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "import sys\n",
    "from typing import Optional, List, Dict, Union\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"../genev2/\")\n",
    "from ProteinEmbedding import ProteinEmbeddings\n",
    "from GraphEmbeddings import GraphEmbeddings\n",
    "from DNAEmbeddings import DNAEmbeddings\n",
    "from IsolateDB import IsolateDB\n",
    "from graph import build_graph_from_edges, laplacian_eigenvectors\n",
    "from Dataset import ViromeDataset, NoisySubset\n",
    "from Transformer import TransformerClassifier\n",
    "from Loss import IntegratedMaskedCrossEntropyLoss\n",
    "from Train import train_val_loop\n",
    "\n",
    "import h5py\n",
    "import sqlite3\n",
    "import faiss\n",
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.cluster import AgglomerativeClustering, HDBSCAN\n",
    "from cuml import KMeans\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.decomposition import PCA\n",
    "from cuml.preprocessing import StandardScaler\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe5190f",
   "metadata": {},
   "source": [
    "### Load file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cd666b-9dc2-4ce4-8211-6c332b8d4fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_path = \"/projects/m000151/khoa/repos/prophage/data/cluster_map.csv\"\n",
    "embeddings_path = \"/projects/m000151/khoa/repos/prophage/data/protein_embeddings.h5\"\n",
    "virus_path = \"/projects/m000151/khoa/repos/prophage/data/virome_db.csv\"\n",
    "genome_path = \"/projects/m000151/khoa/repos/prophage/data/genome_db_new.csv\"\n",
    "isolate_path = \"/projects/m000151/khoa/repos/prophage/data/isolate_db_full.sqlite\"\n",
    "vclust_path = \"/projects/m000151/khoa/repos/prophage/data/cluster_ani30_qcov50_rcov50_set_cover.tsv\"\n",
    "edges_path = \"/projects/m000151/khoa/repos/prophage/data/ecoli_ppi_edges.txt\"\n",
    "ecoli_embeddings = \"/projects/m000151/khoa/repos/prophage/data/ecoli.esmc_embeddings.pkl\"\n",
    "dna_embeddings =  \"/projects/m000151/khoa/repos/prophage/data/dna_features.h5\"\n",
    "dna_embeddings_path =  \"/projects/m000151/khoa/repos/prophage/data/compressed_dnafeat.h5\"\n",
    "protein_embeddings_path = \"/projects/m000151/khoa/repos/prophage/data/compressed_protein_embeddings.h5\"\n",
    "protein_embeddings_og_path = \"/projects/m000151/khoa/repos/prophage/data/protein_embeddings.h5\"\n",
    "graph_embeddings_path = \"/projects/m000151/khoa/repos/prophage/data/graph_embeddings.h5\"\n",
    "graph_protein_cluster_map = \"/projects/m000151/khoa/repos/prophage/data/protein_cluster_graph_map.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e417e",
   "metadata": {},
   "source": [
    "### Prepare embeddings and datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb258c5-0ec8-4709-b349-e3afb410f748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proteinEmbeddings = ProteinEmbeddings(\n",
    "    embeddings_path=protein_embeddings_path,\n",
    "    cluster_map_path=cluster_path\n",
    ")\n",
    "dnaEmbeddings = DNAEmbeddings(\n",
    "    embeddings_path=dna_embeddings_path\n",
    ")\n",
    "graph_tokens = np.load(graph_protein_cluster_map, allow_pickle=True).item()\n",
    "graphEmbeddings = GraphEmbeddings(graph_embeddings_path, cluster_path, graph_tokens)\n",
    "db = IsolateDB(isolate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5e0c8-b8a0-4f01-8ae7-8683562795ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_dataset = ViromeDataset(db, proteinEmbeddings, dnaEmbeddings, graphEmbeddings, max_sequence_length=10_000)\n",
    "train_dataset = NoisySubset(full_dataset, train_indices)\n",
    "val_dataset   = Subset(full_dataset, val_indices)\n",
    "test_dataset  = Subset(full_dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83448d54-1646-4d21-90ae-547e94ff82c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Hyper-parameters\n",
    "NUM_EPOCHS    = 40\n",
    "BATCH_SIZE    = 16\n",
    "MAX_SEQ_LEN   = 10_000\n",
    "GRAD_CLIP     = 1.0\n",
    "DEVICE        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2dc47b",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84b149-ee9d-4983-a76b-0160ea94a323",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"virome-transformer\",\n",
    "    name=f\"model_integrated_loss\",\n",
    "    config={\n",
    "        \"track_embed_dim\": 8,\n",
    "        \"strand_embed_dim\": 2,\n",
    "        \"homologs_embed_dim\": 1,\n",
    "        \"embed_dim\": 256,\n",
    "        \"num_heads\": 4,\n",
    "        \"ffn_dim\": 256,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.2,\n",
    "        \"lr\": 1e-4,\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "        \"epochs\": 30,\n",
    "        \"max_grad_norm\": 0.1,\n",
    "        \"warmup_ratio\": 0.05,\n",
    "        \"protein_embed_dim_shrink\":128,\n",
    "        \"dna_embed_dim_shrink\":128,\n",
    "        \"graph_embed_dim_shrink\":16,\n",
    "        \"unk_bias_ratio\": 0.95\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2) Build model, loss, optimizer\n",
    "model = TransformerClassifier(\n",
    "    type_vocab_size    = len(full_dataset.type_categories),\n",
    "    biotype_vocab_size = len(full_dataset.biotype_categories),\n",
    "    strand_vocab_size  = 2,\n",
    "    track_embed_dim    = wandb.config.track_embed_dim,\n",
    "    strand_embed_dim   = wandb.config.strand_embed_dim,\n",
    "    homologs_embed_dim = wandb.config.homologs_embed_dim,\n",
    "    protein_embed_dim  = proteinEmbeddings.embedding_dim,\n",
    "    dna_embed_dim      = dnaEmbeddings.embedding_dim,\n",
    "    graph_embed_dim    = graphEmbeddings.embedding_dim,\n",
    "    protein_embed_dim_shrink  = wandb.config.protein_embed_dim_shrink,\n",
    "    dna_embed_dim_shrink      = wandb.config.dna_embed_dim_shrink,\n",
    "    graph_embed_dim_shrink    = wandb.config.graph_embed_dim_shrink,\n",
    "    embed_dim          = wandb.config.embed_dim,\n",
    "    num_heads          = wandb.config.num_heads,\n",
    "    ffn_dim            = wandb.config.ffn_dim,\n",
    "    num_layers         = wandb.config.num_layers,\n",
    "    dropout            = wandb.config.dropout,\n",
    "    max_seq_len        = 10_000\n",
    ").to(DEVICE)\n",
    "\n",
    "# loss_fn   = MaskedCrossEntropyLoss(unk_bias_ratio = 0.98, unk_weight = 1.0)\n",
    "loss_fn   = IntegratedMaskedCrossEntropyLoss(unk_bias_ratio = wandb.config.unk_bias_ratio)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "params = [\n",
    "    {\n",
    "        \"params\": [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 1e-5,   # most of the weights\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,    # biases & norm layers\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params, lr=wandb.config.lr)\n",
    "\n",
    "# 3) Create a linear‐warmup + decay scheduler\n",
    "total_steps = len(train_loader) * wandb.config.epochs\n",
    "warmup_steps = int(total_steps * wandb.config.warmup_ratio)\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=warmup_steps,\n",
    "#     num_training_steps=total_steps\n",
    "# )\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    "    num_cycles=0.5,       # one half‐cycle of cosine (optional; default=0.5)\n",
    "    last_epoch=-1\n",
    ")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_name = f\"model_d{wandb.config.embed_dim}_h{wandb.config.num_heads}_l{wandb.config.num_layers}_integrated_losss_{timestamp}\"\n",
    "output_dir = f\"/projects/m000151/khoa/repos/prophage/outs/ckpts/{file_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 4) Run training/validation loop with W&B, gradient clipping, and warm‐up\n",
    "history = train_val_loop(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    DEVICE,\n",
    "    num_epochs=wandb.config.epochs,\n",
    "    checkpoint_prefix=f\"{output_dir}/epoch_\",\n",
    "    use_wandb=True,\n",
    "    max_grad_norm=wandb.config.max_grad_norm,\n",
    "    scheduler=scheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958f6f3",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e43f7-47be-4716-a895-36ca23ab965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d10ae-8e33-418c-8867-c2196e28aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = TransformerClassifier(\n",
    "    type_vocab_size    = len(full_dataset.type_categories),\n",
    "    biotype_vocab_size = len(full_dataset.biotype_categories),\n",
    "    strand_vocab_size  = 2,\n",
    "    track_embed_dim    = wandb.config.track_embed_dim,\n",
    "    strand_embed_dim   = wandb.config.strand_embed_dim,\n",
    "    homologs_embed_dim = wandb.config.homologs_embed_dim,\n",
    "    protein_embed_dim  = proteinEmbeddings.embedding_dim,\n",
    "    dna_embed_dim      = dnaEmbeddings.embedding_dim,\n",
    "    graph_embed_dim    = graphEmbeddings.embedding_dim,\n",
    "    protein_embed_dim_shrink  = wandb.config.protein_embed_dim_shrink,\n",
    "    dna_embed_dim_shrink      = wandb.config.dna_embed_dim_shrink,\n",
    "    graph_embed_dim_shrink    = wandb.config.graph_embed_dim_shrink,\n",
    "    embed_dim          = wandb.config.embed_dim,\n",
    "    num_heads          = wandb.config.num_heads,\n",
    "    ffn_dim            = wandb.config.ffn_dim,\n",
    "    num_layers         = wandb.config.num_layers,\n",
    "    dropout            = wandb.config.dropout,\n",
    "    max_seq_len        = 10_000\n",
    ").to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(\"model_d256_h4_l2_integrated_losss_30.pt\")[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff94a9ac-9311-431b-bf63-7b92c189015b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    start = 30\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    train = history['train']\n",
    "    val   = history['val']\n",
    "    epochs = list(range(1, len(train['loss'])+1))\n",
    "\n",
    "    # Plot bacteria metrics\n",
    "    fig_bacteria, axes_bacteria = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig_bacteria.suptitle('Bacteria Metrics', fontsize=16)\n",
    "\n",
    "    # Loss\n",
    "    sns.lineplot(x=epochs[start:], y=train['loss'][start:], ax=axes_bacteria[0,0], label='Train Loss')\n",
    "    sns.lineplot(x=epochs[start:], y=val['loss'][start:],   ax=axes_bacteria[0,0], label='Val Loss')\n",
    "    axes_bacteria[0,0].set_title('Loss over Batches')\n",
    "\n",
    "    # PR-AUC Bacteria\n",
    "    sns.lineplot(x=epochs[start:], y=train['ap_neg'][start:], ax=axes_bacteria[0,1], label='Train PR-AUC')\n",
    "    sns.lineplot(x=epochs[start:], y=val['ap_neg'][start:],   ax=axes_bacteria[0,1], label='Val PR-AUC')\n",
    "    axes_bacteria[0,1].set_title('PR-AUC')\n",
    "\n",
    "    # Precision & Recall Bacteria\n",
    "    sns.lineplot(x=epochs[start:], y=train['prec_neg'][start:], ax=axes_bacteria[1,0], label='Train Precision')\n",
    "    sns.lineplot(x=epochs[start:], y=val['prec_neg'][start:],   ax=axes_bacteria[1,0], label='Val Precision')\n",
    "    sns.lineplot(x=epochs[start:], y=train['rec_neg'][start:], ax=axes_bacteria[1,0], label='Train Recall')\n",
    "    sns.lineplot(x=epochs[start:], y=val['rec_neg'][start:],   ax=axes_bacteria[1,0], label='Val Recall')\n",
    "    axes_bacteria[1,0].set_title('Precision & Recall')\n",
    "\n",
    "    # Residuals Bacteria\n",
    "    sns.lineplot(x=epochs[start:], y=np.array(train['pred_neg'])[start:] - np.array(train['true_neg'])[start:],\\\n",
    "                 ax=axes_bacteria[1,1], label='Train Residuals')\n",
    "    sns.lineplot(x=epochs[start:], y=np.array(val['pred_neg'])[start:] - np.array(val['true_neg'])[start:],\\\n",
    "                 ax=axes_bacteria[1,1], label='Val Residuals')\n",
    "    axes_bacteria[1,1].set_title('Residuals')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot virus metrics\n",
    "    fig_virus, axes_virus = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig_virus.suptitle('Virus Metrics', fontsize=16)\n",
    "\n",
    "    # Loss\n",
    "    sns.lineplot(x=epochs[start:], y=train['loss'][start:], ax=axes_virus[0,0], label='Train Loss')\n",
    "    sns.lineplot(x=epochs[start:], y=val['loss'][start:],   ax=axes_virus[0,0], label='Val Loss')\n",
    "    axes_virus[0,0].set_title('Loss over Batches')\n",
    "\n",
    "    # PR-AUC Virus\n",
    "    sns.lineplot(x=epochs[start:], y=train['ap_pos'][start:], ax=axes_virus[0,1], label='Train PR-AUC')\n",
    "    sns.lineplot(x=epochs[start:], y=val['ap_pos'][start:],   ax=axes_virus[0,1], label='Val PR-AUC')\n",
    "    axes_virus[0,1].set_title('PR-AUC')\n",
    "\n",
    "    # Precision & Recall Virus\n",
    "    sns.lineplot(x=epochs[start:], y=train['prec_pos'][start:], ax=axes_virus[1,0], label='Train Precision')\n",
    "    sns.lineplot(x=epochs[start:], y=val['prec_pos'][start:],   ax=axes_virus[1,0], label='Val Precision')\n",
    "    sns.lineplot(x=epochs[start:], y=train['rec_pos'][start:], ax=axes_virus[1,0], label='Train Recall')\n",
    "    sns.lineplot(x=epochs[start:], y=val['rec_pos'][start:],   ax=axes_virus[1,0], label='Val Recall')\n",
    "    axes_virus[1,0].set_title('Precision & Recall')\n",
    "\n",
    "    # Residuals Virus\n",
    "    sns.lineplot(x=epochs[start:], y=np.array(train['pred_pos'])[start:] - np.array(train['true_pos'])[start:],\\\n",
    "                 ax=axes_virus[1,1], label='Train Residuals')\n",
    "    sns.lineplot(x=epochs[start:], y=np.array(val['pred_pos'])[start:] - np.array(val['true_pos'])[start:],\\\n",
    "                 ax=axes_virus[1,1], label='Val Residuals')\n",
    "    axes_virus[1,1].set_title('Residuals')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# To visualize:\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e74a6b-e85a-4ddd-b6b4-0f1bdded74e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_on_test(model, test_loader, loss_fn, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Runs the model on test_loader and returns per-token P(class=1), true labels,\n",
    "    and padding masks as 2D arrays of shape (num_samples, seq_length).\n",
    "\n",
    "    Returns:\n",
    "        probabilities: np.ndarray[float] of shape (N, L)  # P(class=1)\n",
    "        true_labels:   np.ndarray[int]   of shape (N, L)\n",
    "        masks:         np.ndarray[int]   of shape (N, L)  # 1 for real tokens, 0 for padding\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_trues = []\n",
    "    all_masks = []\n",
    "    record_ids = []\n",
    "    seq_len = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            # Move inputs to device\n",
    "            inputs = {\n",
    "                'type_ids':    batch['type_track'].to(device),\n",
    "                'biotype_ids': batch['biotype_track'].to(device),\n",
    "                'strand_ids':  batch['strand_track'].to(device),\n",
    "                'homologs':    batch['homologs_track'].to(device),\n",
    "                'protein_emb': batch['protein_embeddings'].to(device),\n",
    "                'dna_emb':     batch['dna_embeddings'].to(device),\n",
    "                'graph_emb':   batch['graph_embeddings'].to(device),\n",
    "                'mask':        batch['padding_mask'].to(device)\n",
    "            }\n",
    "            labels = batch['labels'].to(device)  # (B, L)\n",
    "\n",
    "            # Forward + loss check\n",
    "            logits = model(\n",
    "                inputs['type_ids'], inputs['biotype_ids'], inputs['strand_ids'],\n",
    "                inputs['homologs'], inputs['protein_emb'], inputs['dna_emb'],\n",
    "                inputs['graph_emb'], mask=inputs['mask']\n",
    "            )\n",
    "            test_loss = loss_fn(logits, labels, inputs['mask'])\n",
    "            if torch.isnan(test_loss):\n",
    "                raise RuntimeError(f\"NaN in test loss at batch {batch_idx}\")\n",
    "\n",
    "            # Softmax and take probability of class=1\n",
    "            probs = F.softmax(logits, dim=-1)       # (B, L, C)\n",
    "            prob_pos = probs[:, :, 1]               # (B, L)\n",
    "\n",
    "            # Move to CPU numpy\n",
    "            prob_np   = prob_pos.cpu().numpy()\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            mask_np   = inputs['mask'].cpu().numpy().astype(int)\n",
    "\n",
    "            # Record sequence length\n",
    "            if seq_len is None:\n",
    "                seq_len = prob_np.shape[1]\n",
    "            if prob_np.shape[1] != seq_len:\n",
    "                raise ValueError(f\"Unexpected seq length: got {prob_np.shape[1]}, expected {seq_len}\")\n",
    "\n",
    "            all_probs.append(prob_np)\n",
    "            all_trues.append(labels_np)\n",
    "            all_masks.append(mask_np)\n",
    "            record_ids.extend(batch[\"record_id\"])\n",
    "\n",
    "    # Stack batches → (N_total, L)\n",
    "    probabilities = np.vstack(all_probs)\n",
    "    true_labels   = np.vstack(all_trues)\n",
    "    masks         = np.vstack(all_masks)\n",
    "\n",
    "    return probabilities, true_labels, masks, record_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0737075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_token_predictions(predictions, true_labels, masks, record_ids,\n",
    "                                num_samples=5, title=\"\", figname=\"\", sort_order = \"descending\"):\n",
    "    \"\"\"\n",
    "    Overlay true vs. predicted token labels for up to `num_samples` sequences.\n",
    "    Here, mask==1 marks tokens to ignore, so only mask==0 positions are counted.\n",
    "    Samples are sorted by their count of valid tokens (mask==0), descending,\n",
    "    and each subplot spans exactly those valid tokens.\n",
    "    \"\"\"\n",
    "    # Compute valid token counts (mask==0)\n",
    "    valid_counts = (masks == 0).sum(axis=1)\n",
    "    # Sort indices by descending valid token count\n",
    "    if sort_order == \"descending\":\n",
    "        sorted_idx = np.argsort(valid_counts)[::-1]\n",
    "    else:\n",
    "        sorted_idx = np.argsort(valid_counts)\n",
    "    selected = sorted_idx[:num_samples]\n",
    "    fig, axes = plt.subplots(len(selected), 1,\n",
    "                             figsize=(12, 2 * len(selected)),\n",
    "                             sharex=False)\n",
    "    if len(selected) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, idx in zip(axes, selected):\n",
    "        L = valid_counts[idx]  # number of tokens where mask==0\n",
    "        x = np.arange(L)\n",
    "\n",
    "        # Take only the first L tokens (assuming masked==1 are trailing pads)\n",
    "        true_vals = true_labels[idx, :L]\n",
    "        pred_vals = predictions[idx, :L]\n",
    "\n",
    "        ax.plot(x, true_vals, label='True label',\n",
    "                alpha=0.5, marker='o', linestyle='-')\n",
    "        ax.plot(x, pred_vals, label='Predicted label',\n",
    "                alpha=0.5, linestyle='-')\n",
    "\n",
    "        ax.set_xlim(0, L-1)\n",
    "        ax.set_ylim(-1.1, 1.1)\n",
    "        ax.set_yticks([-1, 0, 1])\n",
    "        ax.set_title(f\"Sample {record_ids[idx]} (valid tokens={L})\")\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.xlabel('Token position')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    if figname:\n",
    "        plt.savefig(f\"{figname}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545e877-bed9-41e8-a61e-bea80472fcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds, test_trues, test_masks, test_record_ids = evaluate_on_test(best_model, test_loader, loss_fn, device=\"cuda\")\n",
    "val_preds, val_trues, val_masks, val_record_ids = evaluate_on_test(best_model, val_loader, loss_fn, device=\"cuda\")\n",
    "train_preds, train_trues, train_masks, train_record_ids = evaluate_on_test(best_model, train_loader, loss_fn, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d71e8e-2e60-4dd0-8f66-e9938fd0025f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.savez(f\"test_out_{file_name}.npz\", prediction = test_preds, label = test_trues, mask = test_masks, record_id = test_record_ids)\n",
    "np.savez(f\"train_out_{file_name}.npz\", prediction = train_preds, label = train_trues, mask = train_masks, record_id = train_record_ids)\n",
    "np.savez(f\"val_out_{file_name}.npz\", prediction = val_preds, label = val_trues, mask = val_masks, record_id = val_record_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c00e7-33be-40c9-bbfb-f245a0b3d05d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) First, gather all the smoothed sequences that pass your criteria\n",
    "regions = []  # will hold tuples of (smooth_array, record_id)\n",
    "for i, pred in enumerate(preds):\n",
    "    seq = pred[masks[i] == 0]\n",
    "    if seq.size <= 500:\n",
    "        continue\n",
    "\n",
    "    smooth = seq #moving_average(seq, N=1)\n",
    "    if smooth.max() < 0.5:\n",
    "        continue\n",
    "\n",
    "    regions.append((smooth, record_ids[i]))\n",
    "\n",
    "n = len(regions)\n",
    "if n == 0:\n",
    "    print(\"No high-prob regions found.\")\n",
    "else:\n",
    "    # 2) Compute a “nice” grid shape\n",
    "    ncols = int(math.ceil(math.sqrt(n)))\n",
    "    nrows = int(math.ceil(n / ncols))\n",
    "\n",
    "    # 3) Create subplots\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4*ncols, 3*nrows), squeeze=False)\n",
    "\n",
    "    # 4) Plot each region in its own axes\n",
    "    for ax, (smooth, rid) in zip(axes.flat, regions):\n",
    "        ax.plot(smooth)\n",
    "        ax.set_title(rid, fontsize='small')\n",
    "        ax.set_xlabel(\"Position\")\n",
    "        ax.set_ylabel(\"Probability\")\n",
    "\n",
    "    # 5) Turn off any unused subplots\n",
    "    for ax in axes.flat[n:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(f\"{n} High-Probability Regions\", y=1.02)\n",
    "    fig.savefig(\"grid_highprob_0602.png\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\"Plotted\", n, \"regions in a\", nrows, \"×\", ncols, \"grid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081f280-e041-4bcc-b333-221a4de9ecb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_token_predictions(val_preds, val_trues, val_masks, val_record_ids, num_samples=100,\\\n",
    "                            title=\"Testset: True vs Predicted Labels\", \\\n",
    "                            figname=f\"{file_name}_val_prediction_ascending\", sort_order = \"ascending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfbb506-c4cb-4c8d-a599-c7dc05271369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_probability_boxplots(\n",
    "    model,\n",
    "    loader,\n",
    "    device,\n",
    "    num_batches: int = 1,\n",
    "    title: str = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots violin plots of predicted positive-class probabilities stratified by true label groups:\n",
    "    Negative (0), Positive (1), Unknown (-1), and Padding.\n",
    "\n",
    "    Args:\n",
    "        model: trained TransformerClassifier\n",
    "        loader: DataLoader\n",
    "        device: torch device\n",
    "        num_batches: how many batches to include\n",
    "        title: plot title\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    import pandas as pd\n",
    "\n",
    "    model.eval()\n",
    "    records = []\n",
    "    it = iter(loader)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_batches):\n",
    "            batch = next(it)\n",
    "            # move to device\n",
    "            inputs = {\n",
    "                'type_ids':    batch['type_track'].to(device),\n",
    "                'biotype_ids': batch['biotype_track'].to(device),\n",
    "                'strand_ids':  batch['strand_track'].to(device),\n",
    "                'homologs':    batch['homologs_track'].to(device),\n",
    "                'protein_emb': batch['protein_embeddings'].to(device),\n",
    "                'dna_emb':     batch['dna_embeddings'].to(device),\n",
    "                'graph_emb':   batch['graph_embeddings'].to(device),\n",
    "                'mask':        batch['padding_mask'].to(device)\n",
    "            }\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                inputs['type_ids'], inputs['biotype_ids'], inputs['strand_ids'],\n",
    "                inputs['homologs'], inputs['protein_emb'], inputs['dna_emb'], inputs['graph_emb'],\n",
    "                mask=inputs['mask']\n",
    "            )\n",
    "            probs = F.softmax(logits, dim=-1).view(-1, 2)\n",
    "\n",
    "            flat_labels = labels.view(-1)\n",
    "            flat_mask   = inputs['mask'].view(-1)\n",
    "\n",
    "            # gather data\n",
    "            for idx in range(probs.size(0)):\n",
    "                if flat_mask[idx]:\n",
    "                    group = 'Padding'\n",
    "                else:\n",
    "                    true = flat_labels[idx].item()\n",
    "                    if true == 1:\n",
    "                        group = 'Positive (1)'\n",
    "                    elif true == 0:\n",
    "                        group = 'Negative (0)'\n",
    "                    else:\n",
    "                        group = 'Unknown (-1)'\n",
    "                prob_pos = probs[idx, 1].item()\n",
    "                records.append({'TrueGroup': group, 'ProbPos': prob_pos})\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(8,6))\n",
    "    order = ['Negative (0)', 'Positive (1)', 'Unknown (-1)', 'Padding']\n",
    "    sns.stripplot(x='TrueGroup', y='ProbPos', data=df, order=order, alpha = 0.1)\n",
    "    plt.title(title or 'Predicted Positive Probability by True Label Group')\n",
    "    plt.xlabel('True Label Group')\n",
    "    plt.ylabel('Predicted Probability (class=1)')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9bbee-31a5-41ee-a914-df5c1afc5c22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_probability_boxplots(best_model, train_loader, DEVICE, num_batches=3, title=\"Train Probabilities\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prophage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
